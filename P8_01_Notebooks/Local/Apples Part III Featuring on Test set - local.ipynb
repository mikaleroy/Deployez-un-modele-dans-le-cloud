{
 "cells": [
  {
   "cell_type": "raw",
   "id": "22536cc5",
   "metadata": {},
   "source": [
    "conda list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d20bafdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from PIL import ImageEnhance\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import io\n",
    "\n",
    "import os\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql.functions import col, pandas_udf, udf, PandasUDFType, size\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "\n",
    "from pyspark.ml.feature import StandardScaler, PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c42c924",
   "metadata": {},
   "source": [
    "# Create spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1239a41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a spark session\n",
    "spark = (SparkSession.builder\n",
    ".master('local[2]')\n",
    ".appName('Test featuring')\n",
    ".config('spark.driver.extraClassPath', \n",
    "        '/home/demo/hadoop/hadoop-3.2.2/share/hadoop/tools/lib/aws-java-sdk-bundle-1.11.375.jar:/home/demo/hadoop/hadoop-3.2.2/share/hadoop/tools/lib/hadoop-aws-3.2.0.jar:/home/demo/spark-avro_2.11:4.0.0.jar')         \n",
    ".config('spark.executor.heartbeatInterval', '800000')\n",
    ".config('spark.network.timeout', '900000')  \n",
    ".config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    ".config(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"128\") \n",
    ".getOrCreate()\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9fb3f8",
   "metadata": {},
   "source": [
    "# Load images from local storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91a07bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_images = 'fruits-360/Test_apples/'\n",
    "path_offset = len('file:/mnt/c/Users/demo/Desktop/Projet08/'+path_to_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1122f6a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- path: string (nullable = true)\n",
      " |-- modificationTime: timestamp (nullable = true)\n",
      " |-- length: long (nullable = true)\n",
      " |-- content: binary (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "images =(spark\n",
    "         .read\n",
    "         .format(\"binaryFile\")\n",
    "         .option(\"pathGlobFilter\", \"*.jpg\")\n",
    "         .option(\"recursiveFileLookup\", \"true\")\n",
    "         .load(path_to_images)\n",
    "        )\n",
    "\n",
    "images.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "817b20ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of images in train set 805\n"
     ]
    }
   ],
   "source": [
    "# Total number of images\n",
    "totalMunber = images.count()\n",
    "print('Total number of images in train set {}'.format(totalMunber))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cfa648",
   "metadata": {},
   "source": [
    "## Retrieve labels from image path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0eb23d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get only fruit name from path\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql import types \n",
    "\n",
    "col_label = udf(lambda s : extract_label(s), types.StringType())\n",
    "\n",
    "def extract_label(s):\n",
    "    last = s[path_offset :]\n",
    "    return last[:last.rfind('/')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7f31824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- path: string (nullable = true)\n",
      " |-- modificationTime: timestamp (nullable = true)\n",
      " |-- length: long (nullable = true)\n",
      " |-- content: binary (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "images = images.withColumn('label',col_label(images.path))\n",
    "images.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aba7eeaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|label         |\n",
      "+--------------+\n",
      "|Apple Braeburn|\n",
      "|Apple Braeburn|\n",
      "|Apple Braeburn|\n",
      "|Apple Braeburn|\n",
      "|Apple Braeburn|\n",
      "|Apple Braeburn|\n",
      "|Apple Braeburn|\n",
      "|Apple Braeburn|\n",
      "|Apple Braeburn|\n",
      "|Apple Braeburn|\n",
      "|Apple Braeburn|\n",
      "|Apple Braeburn|\n",
      "|Apple Braeburn|\n",
      "|Apple Braeburn|\n",
      "|Apple Braeburn|\n",
      "|Apple Red 1   |\n",
      "|Apple Braeburn|\n",
      "|Apple Red 1   |\n",
      "|Apple Braeburn|\n",
      "|Apple Braeburn|\n",
      "+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Get only fruit name from path\n",
    "images.select('label').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fedb107a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "By label images count :\n",
      "+---------------+-----+\n",
      "|          label|count|\n",
      "+---------------+-----+\n",
      "| Apple Golden 3|  161|\n",
      "|    Apple Red 2|  164|\n",
      "|Apple Pink Lady|  152|\n",
      "|    Apple Red 1|  164|\n",
      "| Apple Braeburn|  164|\n",
      "+---------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# By label count\n",
    "print('By label images count :')\n",
    "images.groupBy('label').count().show(13)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf5cd95",
   "metadata": {},
   "source": [
    "# Images enhancement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "046c1365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhance image\n",
    "def enhance(img,\n",
    "            color = 1.25,\n",
    "            sharpness = 4.5,\n",
    "            contrast = 1.25,\n",
    "            brigthness= 1.5):\n",
    "    colorEnhancer = ImageEnhance.Color(img)\n",
    "    img = colorEnhancer.enhance(color)\n",
    "    \n",
    "    sharpnessEnhancer = ImageEnhance.Sharpness(img)\n",
    "    sharpnessEnhancer.enhance(sharpness)\n",
    "    \n",
    "    contrastEnhancer = ImageEnhance.Contrast(img)\n",
    "    contrastEnhancer.enhance(contrast)\n",
    "    \n",
    "    brigthnessEnhancer = ImageEnhance.Brightness(img)\n",
    "    brigthnessEnhancer.enhance(brigthness)\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c28d03",
   "metadata": {},
   "source": [
    "# Transfert learning (Resnet50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aba9145e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn():\n",
    "    \"\"\"\n",
    "    Returns a ResNet50 model with top layer removed and broadcasted pretrained weights.\n",
    "    \"\"\"\n",
    "    \n",
    "    resnet_full = ResNet50()\n",
    "\n",
    "    resnet = Model(inputs = resnet_full.inputs,\n",
    "                   outputs = resnet_full.layers[-2].output)\n",
    "    return resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dfd87e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(content):\n",
    "    \"\"\"\n",
    "    Preprocesses raw image bytes for prediction.\n",
    "    \"\"\"\n",
    "    # load raw image from dataframe and resize it to ResNet specifications\n",
    "    img = Image.open(io.BytesIO(content)).resize([224, 224])\n",
    "    # Enhance image\n",
    "    img = enhance(img)\n",
    "    # image to Tensor array\n",
    "    arr = img_to_array(img)\n",
    "    # return ResNet50 preprocessed image\n",
    "    return preprocess_input(arr)\n",
    "\n",
    "\n",
    "def featurize_series(model, content_series):\n",
    "    \"\"\"\n",
    "    Featurize a pd.Series of raw images using the input model.\n",
    "    :return: a pd.Series of image features\n",
    "    \"\"\"\n",
    "    #   input = np.stack(content_series.map(preprocess))\n",
    "    input = tf.convert_to_tensor(np.stack(content_series.map(preprocess)), dtype=tf.float32)\n",
    "    # features from image\n",
    "    preds = model.predict(input)\n",
    "    # For some layers, output features will be multi-dimensional tensors.\n",
    "    # We flatten the feature tensors to vectors for easier storage in Spark DataFrames.\n",
    "    output = [p.flatten() for p in preds]\n",
    "    # return features vector\n",
    "    return pd.Series(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a897f91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterator\n",
    "\n",
    "@pandas_udf('array<float>')\n",
    "def featurize_udf(content_series_iter: Iterator[pd.Series]) -> Iterator[pd.Series]:\n",
    "  '''\n",
    "  This method is a Scalar Iterator pandas UDF wrapping our featurization function.\n",
    "  The decorator specifies that this returns a Spark DataFrame column of type ArrayType(FloatType).\n",
    "  \n",
    "  :param content_series_iter: This argument is an iterator over batches of data, where each batch\n",
    "                              is a pandas Series of image data.\n",
    "  ''' \n",
    "  # With Scalar Iterator pandas UDFs, we can load the model once and then re-use it\n",
    "  # for multiple data batches.  This amortizes the overhead of loading big models.\n",
    "  model = model_fn()\n",
    "  for content_series in content_series_iter:\n",
    "    yield featurize_series(model, content_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16f0c9d",
   "metadata": {},
   "source": [
    "Apply featurization to the DataFrame of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97b10104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- path: string (nullable = true)\n",
      " |-- modificationTime: timestamp (nullable = true)\n",
      " |-- length: long (nullable = true)\n",
      " |-- content: binary (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      " |-- features: array (nullable = true)\n",
      " |    |-- element: float (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Transfert learning \n",
    "images = images.withColumn('features', featurize_udf(images.content))\n",
    "images.printSchema()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "17fc7d76",
   "metadata": {},
   "source": [
    "images.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8343be29",
   "metadata": {},
   "source": [
    "# Standadization and PCA reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "88fa0d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- path: string (nullable = true)\n",
      " |-- modificationTime: timestamp (nullable = true)\n",
      " |-- length: long (nullable = true)\n",
      " |-- content: binary (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      " |-- features: array (nullable = true)\n",
      " |    |-- element: float (containsNull = true)\n",
      " |-- Vect_features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# UDF array -> vector\n",
    "list_to_vector_udf = udf(lambda vs: Vectors.dense([float(i) for i in vs]),\n",
    "                         VectorUDT())\n",
    "# Create new column with vectors\n",
    "images = images.withColumn('Vect_features', list_to_vector_udf(images.features))\n",
    "images.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cbc14b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import PipelineModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af8fc34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduction = PipelineModel.load('Features/Apples PCA reduction.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6407de21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply reduction\n",
    "images = reduction.transform(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b034516",
   "metadata": {},
   "source": [
    "# Vector to array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fb57e14a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- path: string (nullable = true)\n",
      " |-- modificationTime: timestamp (nullable = true)\n",
      " |-- length: long (nullable = true)\n",
      " |-- content: binary (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      " |-- features: array (nullable = true)\n",
      " |    |-- element: float (containsNull = true)\n",
      " |-- Vect_features: vector (nullable = true)\n",
      " |-- Scaled_features: vector (nullable = true)\n",
      " |-- PCA_features: vector (nullable = true)\n",
      " |-- feat_array: array (nullable = false)\n",
      " |    |-- element: double (containsNull = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Transform denseVector to array\n",
    "\n",
    "images = images.withColumn('feat_array', vector_to_array('PCA_features'))\n",
    "images.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9e5393",
   "metadata": {},
   "source": [
    "# Enregistrement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3486e4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "(images\n",
    " .select('path','label','feat_array')\n",
    " .write\n",
    " .partitionBy('label')\n",
    " .mode(\"overwrite\")\n",
    " .parquet(\"Features/Apples-by-label-Test-featured-reducted.parquet\")\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95364d66",
   "metadata": {},
   "source": [
    "# End Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5ea55eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
