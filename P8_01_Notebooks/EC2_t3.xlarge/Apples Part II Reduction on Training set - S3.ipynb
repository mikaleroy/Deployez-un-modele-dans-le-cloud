{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2816e171",
   "metadata": {},
   "source": [
    "This Notebook steps :\n",
    "    \n",
    "   * Create a Spark session\n",
    "    \n",
    "   * Import training images featured, in a Spark DataFrame\n",
    "    \n",
    "   * Project features on 2048 PCA dim \n",
    "    \n",
    "   * Retrive number of significant eigenvalues\n",
    "    \n",
    "   * Apply PCA reduction dimension\n",
    "    \n",
    "   * Stores path, label and PCA reducted feature array partitionned by label in parquet format file on S3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba0139af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# import io\n",
    "\n",
    "# import os\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql.functions import  udf\n",
    "# col, pandas_udf,, PandasUDFType\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "\n",
    "from pyspark.ml.feature import StandardScaler, PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27bc59b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "WORKERS = 'local[2]'\n",
    "\n",
    "LOAD_PATH = 's3a://fruits-images-proceded/Training_apples_featured.parquet'\n",
    "\n",
    "SAVE_PATH = 's3a://fruits-images-proceded/Training_apples_featured-reducted.parquet'\n",
    "\n",
    "MODEL_PATH = 's3a://pca-reduction-model/Apples PCA reduction.model'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92401740",
   "metadata": {},
   "source": [
    "# Create spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2629b970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a spark session\n",
    "spark = (SparkSession.builder\n",
    ".master(WORKERS)\n",
    ".appName('PCA Reduction')\n",
    ".config('spark.driver.extraClassPath', \n",
    "        '/home/ec2-user/hadoop/share/hadoop/tools/lib/aws-java-sdk-bundle-1.11.375.jar:/home/ec2-user/hadoop/share/hadoop/tools/lib/hadoop-aws-3.2.0.jar')         \n",
    ".config('spark.executor.heartbeatInterval', '300000')\n",
    ".config('spark.network.timeout', '900000')\n",
    ".config('spark.sql.execution.arrow.pyspark.enabled', 'true')\n",
    ".config('spark.sql.execution.arrow.maxRecordsPerBatch', '128')\n",
    ".getOrCreate()\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e594fecc",
   "metadata": {},
   "source": [
    "# Load features from local storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b26afef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- path: string (nullable = true)\n",
      " |-- features: array (nullable = true)\n",
      " |    |-- element: float (containsNull = true)\n",
      " |-- label: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "images =(spark\n",
    "         .read\n",
    "         .format('parquet')\n",
    "         .load(LOAD_PATH)\n",
    "        )\n",
    "\n",
    "images.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba86678",
   "metadata": {},
   "source": [
    "# PCA on features\n",
    "Steps :\n",
    "\n",
    "* Transform array of features to dense vector affected to column vect_features\n",
    "* Create Pipeline with stages :\n",
    "    - Standardize with centering reduction\n",
    "    - PCA\n",
    "* Fit PCA\n",
    "* Set dimension \n",
    "* Apply dimension reduction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "374b7a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- path: string (nullable = true)\n",
      " |-- features: array (nullable = true)\n",
      " |    |-- element: float (containsNull = true)\n",
      " |-- label: string (nullable = true)\n",
      " |-- Vect_features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# UDF array -> vector\n",
    "list_to_vector_udf = udf(lambda vs: Vectors.dense([float(i) for i in vs]),\n",
    "                         VectorUDT())\n",
    "# Create new column with vectors\n",
    "images = images.withColumn('Vect_features', list_to_vector_udf(images.features))\n",
    "images.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5a233e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline stages\n",
    "steps = []\n",
    "\n",
    "# standardize features\n",
    "scaler = StandardScaler(inputCol = 'Vect_features', \n",
    "                        outputCol = 'Scaled_features',\n",
    "                        withMean = True,\n",
    "                        withStd = True\n",
    "                       )\n",
    "steps += [scaler]\n",
    "\n",
    "# PCA \n",
    "pca = PCA(k=2048,\n",
    "          inputCol='Scaled_features',\n",
    "          outputCol='PCA_features')\n",
    "steps += [pca]\n",
    "\n",
    "# pipeline \n",
    "pipeline = Pipeline(stages = steps)\n",
    "\n",
    "# Fit the model pipeline\n",
    "reduction = pipeline.fit(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56600d95",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183 principal components explains 90.041399% of variance\n"
     ]
    }
   ],
   "source": [
    "VARIANCE_TO_EXPLAIN = 0.90\n",
    "\n",
    "# Components explained variance\n",
    "explained = reduction.stages[-1].explainedVariance.toArray()\n",
    "\n",
    "# Find the components number to explain 90% of total variance\n",
    "finished = False\n",
    "componentsNum = 0\n",
    "while not finished:\n",
    "    componentsNum += 1\n",
    "    variance = np.sum(explained[0:componentsNum])\n",
    "    finished = (variance > VARIANCE_TO_EXPLAIN)\n",
    "    \n",
    "print('{} principal components explains {:%} of variance'.format(componentsNum,\n",
    "                                                                np.cumsum(explained[0:componentsNum])[-1]\n",
    "                                                               ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a4e091a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting k to componentsNum and refit pipeline\n",
    "params={pca.k : componentsNum}\n",
    "reduction = pipeline.fit(images, params= params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27cbf930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- path: string (nullable = true)\n",
      " |-- features: array (nullable = true)\n",
      " |    |-- element: float (containsNull = true)\n",
      " |-- label: string (nullable = true)\n",
      " |-- Vect_features: vector (nullable = true)\n",
      " |-- Scaled_features: vector (nullable = true)\n",
      " |-- PCA_features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply reduction\n",
    "images = reduction.transform(images)\n",
    "images.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ecd6ec77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- path: string (nullable = true)\n",
      " |-- features: array (nullable = true)\n",
      " |    |-- element: float (containsNull = true)\n",
      " |-- label: string (nullable = true)\n",
      " |-- Vect_features: vector (nullable = true)\n",
      " |-- Scaled_features: vector (nullable = true)\n",
      " |-- PCA_features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "images.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b6ba6ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- path: string (nullable = true)\n",
      " |-- features: array (nullable = true)\n",
      " |    |-- element: float (containsNull = true)\n",
      " |-- label: string (nullable = true)\n",
      " |-- Vect_features: vector (nullable = true)\n",
      " |-- Scaled_features: vector (nullable = true)\n",
      " |-- PCA_features: vector (nullable = true)\n",
      " |-- feat_array: array (nullable = false)\n",
      " |    |-- element: double (containsNull = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Transform denseVector to array\n",
    "images = images.withColumn('feat_array', vector_to_array('PCA_features'))\n",
    "images.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e433df13",
   "metadata": {},
   "source": [
    "# Enregistrement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fcb93bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save reduction pipeline \n",
    "reduction.write().overwrite().save(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8ba9500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply standardization and save to parquet, partitionned by label\n",
    "(images\n",
    " .select('path','label','feat_array')\n",
    " .write\n",
    " .partitionBy('label')\n",
    " .mode('overwrite')\n",
    " .parquet(SAVE_PATH)\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee882bc",
   "metadata": {},
   "source": [
    "# End Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "355db7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b912d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
