{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56d3b8a7",
   "metadata": {},
   "source": [
    "This Notebook steps :\n",
    "    \n",
    "   * Create a Spark session\n",
    "    \n",
    "   * Import training images to proceed, as binary, in a Spark DataFrame\n",
    "    \n",
    "   * Labeling images, by fruit name, extracted from images path \n",
    "    \n",
    "   * Enhance image by tweeking color, sharpness, contrast, brightness\n",
    "    \n",
    "   * Extract 2048 features array by tranfert learning, using Keras Resnet50 CNN\n",
    "    \n",
    "   * Apply PCA reduction dimension\n",
    "    \n",
    "   * Stores path, label and PCA reducted feature array partitionned by label in parquet format file on S3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90e1c57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from PIL import ImageEnhance\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import io\n",
    "\n",
    "import os\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql.functions import col, pandas_udf, udf, PandasUDFType, size\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "\n",
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "\n",
    "from pyspark.ml.feature import StandardScaler, PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af555596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "WORKERS = 'local[2]'\n",
    "\n",
    "LOAD_PATH = 's3a://fruits-images-to-proceed/Test_apples/'\n",
    "\n",
    "SAVE_PATH = 's3a://fruits-images-proceded/Test_apples_featured-reducted.parquet'\n",
    "\n",
    "MODEL_PATH = 's3a://pca-reduction-model/Apples PCA reduction.model'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb4ed4d",
   "metadata": {},
   "source": [
    "# Create spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54d7afbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a spark session\n",
    "spark = (SparkSession.builder\n",
    ".master(WORKERS)\n",
    ".appName('Test featuring on apples')\n",
    ".config('spark.driver.extraClassPath', \n",
    "        '/home/ec2-user/hadoop/share/hadoop/tools/lib/aws-java-sdk-bundle-1.11.375.jar:/home/ec2-user/hadoop/share/hadoop/tools/lib/hadoop-aws-3.2.0.jar')         \n",
    ".config('spark.executor.heartbeatInterval', '300000')\n",
    ".config('spark.network.timeout', '900000') \n",
    ".config('spark.sql.execution.arrow.pyspark.enabled', 'true')\n",
    ".config('spark.sql.execution.arrow.maxRecordsPerBatch', '128')\n",
    ".getOrCreate()\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13de35d5",
   "metadata": {},
   "source": [
    "# Load images from local storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3302fa5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- path: string (nullable = true)\n",
      " |-- modificationTime: timestamp (nullable = true)\n",
      " |-- length: long (nullable = true)\n",
      " |-- content: binary (nullable = true)\n",
      "\n",
      "CPU times: user 4 ms, sys: 0 ns, total: 4 ms\n",
      "Wall time: 3.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "images =(spark\n",
    "         .read\n",
    "         .format('binaryFile')\n",
    "         .option('pathGlobFilter', '*.jpg')\n",
    "         .option('recursiveFileLookup', 'true')\n",
    "         .load(LOAD_PATH)\n",
    "        )\n",
    "\n",
    "images.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce0333e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of images in train set 805\n"
     ]
    }
   ],
   "source": [
    "# Total number of images\n",
    "totalMunber = images.count()\n",
    "print('Total number of images in train set {}'.format(totalMunber))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fda9f9",
   "metadata": {},
   "source": [
    "## Retrieve labels from image path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6827562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Offset of starting image name\n",
    "path_offset = len(LOAD_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7cc42daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get only fruit name from path\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql import types \n",
    "\n",
    "col_label = udf(lambda s : extract_label(s), types.StringType())\n",
    "\n",
    "def extract_label(s):\n",
    "    last = s[path_offset :]\n",
    "    return last[:last.rfind('/')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62e204f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- path: string (nullable = true)\n",
      " |-- modificationTime: timestamp (nullable = true)\n",
      " |-- length: long (nullable = true)\n",
      " |-- content: binary (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "images = images.withColumn('label',col_label(images.path))\n",
    "images.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf998842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|label         |\n",
      "+--------------+\n",
      "|Apple Braeburn|\n",
      "|Apple Braeburn|\n",
      "|Apple Braeburn|\n",
      "|Apple Braeburn|\n",
      "|Apple Braeburn|\n",
      "|Apple Braeburn|\n",
      "|Apple Braeburn|\n",
      "|Apple Braeburn|\n",
      "|Apple Braeburn|\n",
      "|Apple Braeburn|\n",
      "|Apple Braeburn|\n",
      "|Apple Braeburn|\n",
      "|Apple Braeburn|\n",
      "|Apple Braeburn|\n",
      "|Apple Braeburn|\n",
      "|Apple Red 1   |\n",
      "|Apple Braeburn|\n",
      "|Apple Red 1   |\n",
      "|Apple Braeburn|\n",
      "|Apple Braeburn|\n",
      "+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Get only fruit name from path\n",
    "images.select('label').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75853563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "By label images count :\n",
      "+---------------+-----+\n",
      "|          label|count|\n",
      "+---------------+-----+\n",
      "| Apple Golden 3|  161|\n",
      "|    Apple Red 2|  164|\n",
      "|Apple Pink Lady|  152|\n",
      "|    Apple Red 1|  164|\n",
      "| Apple Braeburn|  164|\n",
      "+---------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# By label count\n",
    "print('By label images count :')\n",
    "images.groupBy('label').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a426d1b2",
   "metadata": {},
   "source": [
    "# Images enhancement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b0ea27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhance image\n",
    "def enhance(img,\n",
    "            color = 1.25,\n",
    "            sharpness = 4.5,\n",
    "            contrast = 1.25,\n",
    "            brigthness= 1.5):\n",
    "    colorEnhancer = ImageEnhance.Color(img)\n",
    "    img = colorEnhancer.enhance(color)\n",
    "    \n",
    "    sharpnessEnhancer = ImageEnhance.Sharpness(img)\n",
    "    sharpnessEnhancer.enhance(sharpness)\n",
    "    \n",
    "    contrastEnhancer = ImageEnhance.Contrast(img)\n",
    "    contrastEnhancer.enhance(contrast)\n",
    "    \n",
    "    brigthnessEnhancer = ImageEnhance.Brightness(img)\n",
    "    brigthnessEnhancer.enhance(brigthness)\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460f16ef",
   "metadata": {},
   "source": [
    "# Transfert learning (Resnet50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2127e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn():\n",
    "    '''\n",
    "    Returns a ResNet50 model with top layer removed and broadcasted pretrained weights.\n",
    "    '''\n",
    "    \n",
    "    resnet_full = ResNet50()\n",
    "\n",
    "    resnet = Model(inputs = resnet_full.inputs,\n",
    "                   outputs = resnet_full.layers[-2].output)\n",
    "    return resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c235529",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(content):\n",
    "    '''\n",
    "    Preprocesses raw image bytes for prediction.\n",
    "    '''\n",
    "    # load raw image from dataframe and resize it to ResNet specifications\n",
    "    img = Image.open(io.BytesIO(content)).resize([224, 224])\n",
    "    # Enhance image\n",
    "    img = enhance(img)\n",
    "    # image to Tensor array\n",
    "    arr = img_to_array(img)\n",
    "    # return ResNet50 preprocessed image\n",
    "    return preprocess_input(arr)\n",
    "\n",
    "\n",
    "def featurize_series(model, content_series):\n",
    "    '''\n",
    "    Featurize a pd.Series of raw images using the input model.\n",
    "    :return: a pd.Series of image features\n",
    "    '''\n",
    "    #   input = np.stack(content_series.map(preprocess))\n",
    "    input = tf.convert_to_tensor(np.stack(content_series.map(preprocess)), dtype=tf.float32)\n",
    "    # features from image\n",
    "    preds = model.predict(input)\n",
    "    # For some layers, output features will be multi-dimensional tensors.\n",
    "    # We flatten the feature tensors to vectors for easier storage in Spark DataFrames.\n",
    "    output = [p.flatten() for p in preds]\n",
    "    # return features vector\n",
    "    return pd.Series(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b994b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterator\n",
    "\n",
    "@pandas_udf('array<float>')\n",
    "def featurize_udf(content_series_iter: Iterator[pd.Series]) -> Iterator[pd.Series]:\n",
    "  '''\n",
    "  This method is a Scalar Iterator pandas UDF wrapping our featurization function.\n",
    "  The decorator specifies that this returns a Spark DataFrame column of type ArrayType(FloatType).\n",
    "  \n",
    "  :param content_series_iter: This argument is an iterator over batches of data, where each batch\n",
    "                              is a pandas Series of image data.\n",
    "  ''' \n",
    "  # With Scalar Iterator pandas UDFs, we can load the model once and then re-use it\n",
    "  # for multiple data batches.  This amortizes the overhead of loading big models.\n",
    "  model = model_fn()\n",
    "  for content_series in content_series_iter:\n",
    "    yield featurize_series(model, content_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d45baa7",
   "metadata": {},
   "source": [
    "Apply featurization to the DataFrame of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a8eaafeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avoiding Out Of Memory (OOM) errors by reducing the Arrow batch size\n",
    "spark.conf.set('spark.sql.execution.arrow.maxRecordsPerBatch', '12')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0a31d2d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- path: string (nullable = true)\n",
      " |-- modificationTime: timestamp (nullable = true)\n",
      " |-- length: long (nullable = true)\n",
      " |-- content: binary (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      " |-- features: array (nullable = true)\n",
      " |    |-- element: float (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Transfert learning \n",
    "images = images.withColumn('features', featurize_udf(images.content))\n",
    "images.printSchema()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e240867e",
   "metadata": {},
   "source": [
    "images.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd75834",
   "metadata": {},
   "source": [
    "# Standadization and PCA reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57ad03d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- path: string (nullable = true)\n",
      " |-- modificationTime: timestamp (nullable = true)\n",
      " |-- length: long (nullable = true)\n",
      " |-- content: binary (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      " |-- features: array (nullable = true)\n",
      " |    |-- element: float (containsNull = true)\n",
      " |-- Vect_features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# UDF array -> vector\n",
    "list_to_vector_udf = udf(lambda vs: Vectors.dense([float(i) for i in vs]),\n",
    "                         VectorUDT())\n",
    "# Create new column with vectors\n",
    "images = images.withColumn('Vect_features', list_to_vector_udf(images.features))\n",
    "images.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dcc3eb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduction = PipelineModel.load(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "972aad94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply reduction\n",
    "images = reduction.transform(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b515d8a",
   "metadata": {},
   "source": [
    "# Vector to array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2cbb71e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- path: string (nullable = true)\n",
      " |-- modificationTime: timestamp (nullable = true)\n",
      " |-- length: long (nullable = true)\n",
      " |-- content: binary (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      " |-- features: array (nullable = true)\n",
      " |    |-- element: float (containsNull = true)\n",
      " |-- Vect_features: vector (nullable = true)\n",
      " |-- Scaled_features: vector (nullable = true)\n",
      " |-- PCA_features: vector (nullable = true)\n",
      " |-- feat_array: array (nullable = false)\n",
      " |    |-- element: double (containsNull = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Transform denseVector to array\n",
    "\n",
    "images = images.withColumn('feat_array', vector_to_array('PCA_features'))\n",
    "images.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "110e59a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+------+--------------------+--------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                path|   modificationTime|length|             content|         label|            features|       Vect_features|     Scaled_features|        PCA_features|          feat_array|\n",
      "+--------------------+-------------------+------+--------------------+--------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|s3a://fruits-imag...|2021-09-29 08:39:38|  5473|[FF D8 FF E0 00 1...|Apple Braeburn|[1.5647818, 0.324...|[1.56478178501129...|[1.09162302719634...|[-1.9048678493216...|[-1.9048678493216...|\n",
      "+--------------------+-------------------+------+--------------------+--------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "images.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e24a865",
   "metadata": {},
   "source": [
    "# Enregistrement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3e8b0822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Spark DataFrame, partitionned by label, in S3 Bucket\n",
    "(images\n",
    " .select('path','label','feat_array')\n",
    " .write\n",
    " .partitionBy('label')\n",
    " .mode('overwrite')\n",
    " .parquet(SAVE_PATH)\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad62f7d0",
   "metadata": {},
   "source": [
    "# End Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "43dd96b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2db58ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
